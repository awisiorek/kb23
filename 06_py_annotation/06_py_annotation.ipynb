{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 6. Korpusannotation mit stanza\n",
    "\n",
    "- linguistische Annotation von Korpora \n",
    "- Sequenzklassifikation (Wort &rarr; Label) \n",
    "- Tagging und Parsing\n",
    "- Analyseebenen:\n",
    "  - Segmentierung (*EOS, Tokenisierung, Stemming*)\n",
    "  - morphologisch (*Lemmatizer, POS, morphol. Feature*)\n",
    "  - syntaktisch (*Dependencies*)\n",
    "  - semantisch (*NER*)\n",
    "- Pipeline (*Processors*):\n",
    "  - End-of-Sentence\n",
    "  - Tokenization\n",
    "  - Lemmatization\n",
    "  - POS Tagging\n",
    "  - Syntactic Parsing\n",
    "  - Named Entity Recognition\n",
    "  - Sentiment Analysis\n",
    "\n",
    "\n",
    "- Stanza Dokumentation: https://stanfordnlp.github.io/stanza/\n",
    "\n",
    "#### weiterführende Literatur:\n",
    "- https://www.nltk.org/book/ch05.html (Tagging)\n",
    "- https://www.nltk.org/book/ch06.html (Text Classification)\n",
    "- https://www.nltk.org/book/ch07.html (Information Extraction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. stanza \n",
    "\n",
    "- stanza (*Stanford Python NLP Tools*) ist eine Python Library für die linguistische Annotation von Textdaten\n",
    "- hat einen ähnlich Umfang wie die bekannte Python NLP-Library spacy\n",
    "- entwickelt als moderne Python-basierte Alternative zu den bekannten Stanford CoreNLP Annotationstools (Java-basiert)\n",
    "- verwendet neuronale Netze für das Training seiner Tagger und weiterer Klassifikator-Modelle\n",
    "- bietet eine Vielzahl an linguistischen Annotationsebenen, u.a. POS-Tagging, Dependency-Parsing und NER (entity recognition) \n",
    "- es gibt eine Vielzahl an Modellen für verschiedenen Sprachen, diese sind trainiert auf annotierten Korpora im UD-Format ([Universal-Dependencies](https://universaldependencies.org/)): https://stanfordnlp.github.io/stanza/available_models.html\n",
    "- man kann auch eigene stanza Modelle trainieren: https://stanfordnlp.github.io/stanza/training.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Sequenzklassifikation \n",
    "\n",
    "- Annotation als Mapping von sequentiellen linguistischen Einheiten auf Annotationslabel bestimmter linguistischer Analyseebenen\n",
    "- Analysen bauen aufeinander auf (Pipeline), z.B. benötigt Dependency-Parsing POS-Label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.nltk.org/images/tag-context.png\" width=\"44%\">\n",
    "\n",
    "> When we perform a language processing task based on unigrams, we are using one item of **context**. In the case of tagging, we only consider the current token, in isolation from any larger context. Given such a model, the best we can do is tag each word with its a priori most likely tag. This means we would tag a word such as *wind* with the same tag, regardless of whether it appears in the context *the wind* or *to wind*.\n",
    "\n",
    "> An n-gram tagger is a generalization of a unigram tagger whose context is the current word together with the part-of-speech tags of the n-1 preceding tokens, as shown in 5.1. The tag to be chosen, tn, is circled, and the context is shaded in grey. In the example of an n-gram tagger shown in 5.1, we have n=3; that is, we consider the tags of the two preceding words in addition to the current word. An n-gram tagger picks the tag that is most likely in the given context.\n",
    "\n",
    "https://www.nltk.org/book/ch05.html#fig-tag-context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 1 Regelbasierte vs. statistische/neuronale NLP-Modelle\n",
    "\n",
    "- A: Regelbasierte Modelle\n",
    "  - Regeln, die ein Modell spezifizieren\n",
    "  - Beispiele:\n",
    "     - Regelbasierter Tokenizer (siehe https://stanfordnlp.github.io/stanza/tokenize.html#use-spacy-for-fast-tokenization-and-sentence-segmentation)\n",
    "     - Morphologischer Tagger für reguläre Ausdrücke mit NLTK: https://www.nltk.org/book/ch05.html#the-regular-expression-tagger\n",
    "\n",
    "\n",
    "- B: statistische/neuronale NLP-Modelle\n",
    "  - Modelle lernen Regeln (Zuordnung von Eingabe zu Ausgabe) aus Trainingsdaten\n",
    "  - basierend auf Mustern in Trainingsdaten\n",
    "  - verschiedene Arten von ML-Algorithmen:\n",
    "     - Statistische Modelle\n",
    "     - neuronale Netzwerke (Deep Learning)\n",
    " \n",
    " \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Komponenten einer NLP Pipeline\n",
    "\n",
    "> To start annotating text with Stanza, you would typically start by building a `Pipeline` that contains Processors, each fulfilling a specific NLP task you desire (e.g., tokenization, part-of-speech tagging, syntactic parsing, etc). The pipeline takes in raw text or a `Document` object that contains partial annotations, runs the specified processors in succession, and returns an annotated `Document` (see the documentation on `Document` for more information on how to extract these annotations).<br>https://stanfordnlp.github.io/stanza/pipeline.html\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://stanfordnlp.github.io/stanza/assets/images/pipeline.png\" width=74%>\n",
    "\n",
    "*Stanza Pipeline Overview, https://stanfordnlp.github.io/stanza/*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Downstream Tasks\n",
    "\n",
    "- Named Entitiy Recognition (NER)\n",
    "-  Sentiment Analysis\n",
    "-  (Semantic Parsing, e.g. for Question-Answering-Systems)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Beispiel 1: Information-Extraction-Pipeline \n",
    "<img src=\"https://www.nltk.org/images/ie-architecture.png\" width=\"50%\">\n",
    "\n",
    "> Simple Pipeline Architecture for an Information Extraction System. This system takes the raw text of a document as its input, and generates a list of (entity, relation, entity) tuples as its output. For example, given a document that indicates that the company Georgia-Pacific is located in Atlanta, it might generate the tuple `([ORG: 'Georgia-Pacific'] 'in' [LOC: 'Atlanta'])`.\n",
    "\n",
    "https://www.nltk.org/book/ch01.html#fig-sds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beispiel 2: Dialog-System-Pipeline\n",
    "<img src=\"https://www.nltk.org/images/dialogue.png\" width=\"44%\">\n",
    "\n",
    "> Simple Pipeline Architecture for a Spoken Dialogue System: Spoken input (top left) is analyzed, words are recognized, sentences are parsed and interpreted in context, application-specific actions take place (top right); a response is planned, realized as a syntactic structure, then to suitably inflected words, and finally to spoken output; different types of linguistic knowledge inform each stage of the process.\n",
    "\n",
    "https://www.nltk.org/book/ch07.html#fig-ie-architecture:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Verwendung von stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "###  Download der Modelle\n",
    "\n",
    "- Übersicht über die zur Verfügung stehenden Modelle:\n",
    "https://stanfordnlp.github.io/stanza/installation_usage.html\n",
    "\n",
    "- https://stanfordnlp.github.io/stanza/available_models.html\n",
    "\n",
    "\n",
    "- Trainingsdaten im UD-Format: \n",
    "  - auch Ancient Languages Corpora (PROIEL Treebank: https://github.com/proiel )\n",
    "    - z.B. Latin PROIEL Corpus: https://raw.githubusercontent.com/proiel/proiel-treebank/master/latin-nt.conll (Download)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7a6032fcc3540879e14891bcaff3180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 15:08:41 INFO: Downloading default packages for language: en (English) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a37149da87854482bf4772c421f049b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.5.0/models/default.zip:   0%|          | 0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-02 15:09:06 INFO: Finished downloading models and saved to ~/stanza_resources.\n"
     ]
    }
   ],
   "source": [
    "#stanza.download('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stanza.download('en') #English\n",
    "#stanza.download('de') #German"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### stanza's Neural Network Model des German-Tokenizer mit torch inspizieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embeddings.weight': tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.1684,  0.0148, -0.1964,  ..., -0.3455, -0.4633,  0.1623],\n",
       "         [ 0.7807,  0.9993,  0.3276,  ...,  2.1226, -2.0106,  0.1123],\n",
       "         ...,\n",
       "         [-0.8969,  1.6423,  0.1080,  ..., -0.9040, -0.2237,  2.4147],\n",
       "         [ 0.6745, -0.7798, -0.8825,  ..., -0.2660, -1.8693, -1.0810],\n",
       "         [ 0.5926,  0.5096, -1.5175,  ...,  0.6039,  1.0388,  0.2079]]),\n",
       " 'rnn.weight_ih_l0': tensor([[ 0.0177, -0.1476, -0.0815,  ...,  0.0590,  0.0593,  0.0069],\n",
       "         [-0.0261,  0.0572, -0.0376,  ...,  0.0238, -0.0753,  0.2131],\n",
       "         [ 0.1041, -0.0786, -0.1333,  ..., -0.8213, -0.0429,  0.0059],\n",
       "         ...,\n",
       "         [ 0.2776,  0.4871, -0.1580,  ..., -0.6597, -0.1040,  0.0172],\n",
       "         [ 0.0984,  0.1884, -0.2566,  ...,  1.2459, -0.1326,  0.0751],\n",
       "         [-0.2466,  0.0975, -0.1931,  ...,  0.6908, -0.0756, -0.0712]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##inspect neural model (tokenize.pt for de)\n",
    "import torch\n",
    "from os.path import expanduser\n",
    "home = expanduser(\"~\")\n",
    "model_weights = torch.load(home+'/stanza_resources/de/tokenize/gsd.pt',map_location=torch.device('cpu'))\n",
    "#model_weights\n",
    "from itertools import islice\n",
    "dict(islice(model_weights['model'].items(), 0, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline und Modell laden\n",
    "\n",
    "https://stanfordnlp.github.io/stanza/pipeline.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 07:29:41 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| pos       | combined  |\n",
      "| lemma     | combined  |\n",
      "| depparse  | combined  |\n",
      "| sentiment | sstplus   |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2022-01-21 07:29:41 INFO: Use device: cpu\n",
      "2022-01-21 07:29:41 INFO: Loading: tokenize\n",
      "2022-01-21 07:29:41 INFO: Loading: pos\n",
      "2022-01-21 07:29:41 INFO: Loading: lemma\n",
      "2022-01-21 07:29:41 INFO: Loading: depparse\n",
      "2022-01-21 07:29:42 INFO: Loading: sentiment\n",
      "2022-01-21 07:29:42 INFO: Loading: ner\n",
      "2022-01-21 07:29:43 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', download_method=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document-Object\n",
    "\n",
    "#### nlp()\n",
    "\n",
    "- Durchführen einer Annotation auf Satz oder Text erzeugt `Document` Object\n",
    "\n",
    ">  Document object holds the annotation of an entire document, and is automatically generated when a string is annotated by the Pipeline. It contains a collection of Sentences and entities (which are represented as Spans), and can be seamlessly translated into a native Python object.<br>https://stanfordnlp.github.io/stanza/data_objects.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "  [\n",
       "    {\n",
       "      \"id\": 1,\n",
       "      \"text\": \"To\",\n",
       "      \"lemma\": \"to\",\n",
       "      \"upos\": \"PART\",\n",
       "      \"xpos\": \"TO\",\n",
       "      \"head\": 2,\n",
       "      \"deprel\": \"mark\",\n",
       "      \"start_char\": 0,\n",
       "      \"end_char\": 2,\n",
       "      \"ner\": \"O\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": 2,\n",
       "      \"text\": \"be\",\n",
       "      \"lemma\": \"be\",\n",
       "      \"upos\": \"VERB\",\n",
       "      \"xpos\": \"VB\",\n",
       "      \"feats\": \"VerbForm=Inf\",\n",
       "      \"head\": 0,\n",
       "      \"deprel\": \"root\",\n",
       "      \"start_char\": 3,\n",
       "      \"end_char\": 5,\n",
       "      \"ner\": \"O\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": 3,\n",
       "      \"text\": \"or\",\n",
       "      \"lemma\": \"or\",\n",
       "      \"upos\": \"CCONJ\",\n",
       "      \"xpos\": \"CC\",\n",
       "      \"head\": 4,\n",
       "      \"deprel\": \"cc\",\n",
       "      \"start_char\": 6,\n",
       "      \"end_char\": 8,\n",
       "      \"ner\": \"O\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": 4,\n",
       "      \"text\": \"not\",\n",
       "      \"lemma\": \"not\",\n",
       "      \"upos\": \"PART\",\n",
       "      \"xpos\": \"RB\",\n",
       "      \"head\": 2,\n",
       "      \"deprel\": \"conj\",\n",
       "      \"start_char\": 9,\n",
       "      \"end_char\": 12,\n",
       "      \"ner\": \"O\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": 5,\n",
       "      \"text\": \"to\",\n",
       "      \"lemma\": \"to\",\n",
       "      \"upos\": \"PART\",\n",
       "      \"xpos\": \"TO\",\n",
       "      \"head\": 6,\n",
       "      \"deprel\": \"mark\",\n",
       "      \"start_char\": 13,\n",
       "      \"end_char\": 15,\n",
       "      \"ner\": \"O\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": 6,\n",
       "      \"text\": \"be\",\n",
       "      \"lemma\": \"be\",\n",
       "      \"upos\": \"VERB\",\n",
       "      \"xpos\": \"VB\",\n",
       "      \"feats\": \"VerbForm=Inf\",\n",
       "      \"head\": 2,\n",
       "      \"deprel\": \"xcomp\",\n",
       "      \"start_char\": 16,\n",
       "      \"end_char\": 18,\n",
       "      \"ner\": \"O\"\n",
       "    },\n",
       "    {\n",
       "      \"id\": 7,\n",
       "      \"text\": \".\",\n",
       "      \"lemma\": \".\",\n",
       "      \"upos\": \"PUNCT\",\n",
       "      \"xpos\": \".\",\n",
       "      \"head\": 2,\n",
       "      \"deprel\": \"punct\",\n",
       "      \"start_char\": 18,\n",
       "      \"end_char\": 19,\n",
       "      \"ner\": \"O\"\n",
       "    }\n",
       "  ]\n",
       "]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = nlp('To be or not to be.')\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output der Annotationen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: To \tlemma: to\tpos: PART\n",
      "word: be \tlemma: be\tpos: VERB\n",
      "word: or \tlemma: or\tpos: CCONJ\n",
      "word: not \tlemma: not\tpos: PART\n",
      "word: to \tlemma: to\tpos: PART\n",
      "word: be \tlemma: be\tpos: VERB\n",
      "word: . \tlemma: .\tpos: PUNCT\n"
     ]
    }
   ],
   "source": [
    "print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}\\tpos: {word.pos}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processors \n",
    "\n",
    "- durch Angabe der Prozessoren kann eine eigene Pipeline gebaut werden\n",
    "- Prozessoren = Annotationsebenen (*annotation layers*)\n",
    "- optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 07:30:28 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "========================\n",
      "\n",
      "2022-01-21 07:30:28 INFO: Use device: cpu\n",
      "2022-01-21 07:30:28 INFO: Loading: tokenize\n",
      "2022-01-21 07:30:28 INFO: Loading: pos\n",
      "2022-01-21 07:30:28 INFO: Loading: lemma\n",
      "2022-01-21 07:30:28 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,lemma', download_method=None) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ACHTUNG: Pipeline Requirements müssen eingehalten werden!\n",
    "\n",
    "- z.B. benötigt der Lemma-Prozessor den *tokenize*-Prozessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 07:30:31 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| lemma     | combined |\n",
      "========================\n",
      "\n",
      "2022-01-21 07:30:31 INFO: Use device: cpu\n",
      "2022-01-21 07:30:31 INFO: Loading: lemma\n",
      "2022-01-21 07:30:31 INFO: \n",
      "\n"
     ]
    },
    {
     "ename": "PipelineRequirementsException",
     "evalue": "\n\n---\nPipeline Requirements Error!\n\tProcessor: LemmaProcessor\n\tPipeline processors list: lemma\n\tProcessor Requirements: {'tokenize'}\n\t\t- fulfilled: set()\n\t\t- missing: {'tokenize'}\n\nThe processors list provided for this pipeline is invalid.  Please make sure all prerequisites are met for every processor.\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPipelineRequirementsException\u001b[0m             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-d0a0335f19f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstanza\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'lemma'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Library/Python/3.6/lib/python/site-packages/stanza/pipeline/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang, dir, package, processors, logging_level, verbose, use_gpu, model_dir, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpipeline_reqs_exceptions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mPipelineRequirementsException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline_reqs_exceptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done loading processors!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPipelineRequirementsException\u001b[0m: \n\n---\nPipeline Requirements Error!\n\tProcessor: LemmaProcessor\n\tPipeline processors list: lemma\n\tProcessor Requirements: {'tokenize'}\n\t\t- fulfilled: set()\n\t\t- missing: {'tokenize'}\n\nThe processors list provided for this pipeline is invalid.  Please make sure all prerequisites are met for every processor.\n\n\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='lemma', download_method=None) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Tokenisierung\n",
    "\n",
    "- Satz- und Wort-Tokenisierung\n",
    "- Text als Liste von Listen von Wörtern\n",
    "\n",
    "> Tokenization and sentence segmentation in Stanza are jointly performed by the TokenizeProcessor. This processor splits the raw input text into tokens and sentences, so that downstream annotation can happen at the sentence level.<br>https://stanfordnlp.github.io/stanza/tokenize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 07:30:40 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2022-01-21 07:30:40 INFO: Use device: cpu\n",
      "2022-01-21 07:30:40 INFO: Loading: tokenize\n",
      "2022-01-21 07:30:40 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: He \n",
      "word: does \n",
      "word: n't \n",
      "word: stop \n",
      "word: . \n"
     ]
    }
   ],
   "source": [
    "# initialize English neural pipeline\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize', download_method=None)\n",
    "\n",
    "# run annotation:\n",
    "doc = nlp(\"\"\"He doesn't stop. \"\"\")\n",
    "\n",
    "# output:\n",
    "print(*[f'word: {word.text+\" \"}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Word Token (MWT) Expansion (nur für Deutsch, Spanisch, Französisch)\n",
    "- https://stanfordnlp.github.io/stanza/mwt.html\n",
    "- https://stanfordnlp.github.io/CoreNLP/mwt.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 07:30:40 INFO: Loading these models for language: de (German):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "=======================\n",
      "\n",
      "2022-01-21 07:30:40 INFO: Use device: cpu\n",
      "2022-01-21 07:30:40 INFO: Loading: tokenize\n",
      "2022-01-21 07:30:40 INFO: Loading: mwt\n",
      "2022-01-21 07:30:40 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: Ich \n",
      "word: sage \n",
      "word: es \n",
      "word: zu \n",
      "word: dem \n",
      "word: letzen \n",
      "word: Mal \n",
      "word: . \n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='de', processors='tokenize, mwt', download_method=None)\n",
    "doc = nlp(\"\"\"Ich sage es zum letzen Mal.\"\"\")\n",
    "\n",
    "print(*[f'word: {word.text+\" \"}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisierung ohne Satzsegmentierung\n",
    "\n",
    "> Sometimes you might want to tokenize your text given existing sentences (e.g., in machine translation). You can perform tokenization without sentence segmentation, as long as the sentences are split by two continuous newlines (\\n\\n) in the raw text.<br>https://stanfordnlp.github.io/stanza/tokenize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 07:30:40 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2022-01-21 07:30:40 INFO: Use device: cpu\n",
      "2022-01-21 07:30:40 INFO: Loading: tokenize\n",
      "2022-01-21 07:30:40 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Sentence 1 tokens =======\n",
      "id: (1,)\ttext: This\n",
      "id: (2,)\ttext: is\n",
      "id: (3,)\ttext: a\n",
      "id: (4,)\ttext: sentence\n",
      "id: (5,)\ttext: .\n",
      "====== Sentence 2 tokens =======\n",
      "id: (1,)\ttext: This\n",
      "id: (2,)\ttext: is\n",
      "id: (3,)\ttext: a\n",
      "id: (4,)\ttext: second\n",
      "id: (5,)\ttext: .\n",
      "id: (6,)\ttext: This\n",
      "id: (7,)\ttext: is\n",
      "id: (8,)\ttext: a\n",
      "id: (9,)\ttext: third\n",
      "id: (10,)\ttext: .\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize', tokenize_no_ssplit=True, download_method=None)\n",
    "doc = nlp('This is a sentence.\\n\\nThis is a second. This is a third.')\n",
    "\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Start mit Pretokenized Text\n",
    "\n",
    "> In some cases, you might have already tokenized your text, and just want to use Stanza for downstream processing. In these cases, you can feed in pretokenized (and sentence split) text to the pipeline, as newline (\\n) separated sentences, where each sentence is space separated tokens.<br>https://stanfordnlp.github.io/stanza/tokenize.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 07:30:40 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "========================\n",
      "\n",
      "2022-01-21 07:30:40 INFO: Use device: cpu\n",
      "2022-01-21 07:30:40 INFO: Loading: tokenize\n",
      "2022-01-21 07:30:40 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Sentence 1 tokens =======\n",
      "id: (1,)\ttext: This\n",
      "id: (2,)\ttext: is\n",
      "id: (3,)\ttext: token.ization\n",
      "id: (4,)\ttext: done\n",
      "id: (5,)\ttext: my\n",
      "id: (6,)\ttext: way!\n",
      "====== Sentence 2 tokens =======\n",
      "id: (1,)\ttext: Sentence\n",
      "id: (2,)\ttext: split,\n",
      "id: (3,)\ttext: too!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize', tokenize_pretokenized=True, download_method=None)\n",
    "doc = nlp('This is token.ization done my way!\\nSentence split, too!')\n",
    "\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    print(*[f'id: {token.id}\\ttext: {token.text}' for token in sentence.tokens], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Lemmatisierung\n",
    "\n",
    "https://stanfordnlp.github.io/stanza/lemma.html\n",
    "\n",
    "### *Requirements:* `tokenize, mwt, pos`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 07:30:40 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| lemma     | combined |\n",
      "========================\n",
      "\n",
      "2022-01-21 07:30:40 INFO: Use device: cpu\n",
      "2022-01-21 07:30:40 INFO: Loading: tokenize\n",
      "2022-01-21 07:30:40 INFO: Loading: lemma\n",
      "2022-01-21 07:30:40 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: Call \tlemma: call\n",
      "word: me \tlemma: I\n",
      "word: Ishmael \tlemma: ishmael\n",
      "word: . \tlemma: .\n",
      "word: Some \tlemma: some\n",
      "word: years \tlemma: year\n",
      "word: ago \tlemma: ago\n",
      "word: — \tlemma: —\n",
      "word: never \tlemma: never\n",
      "word: mind \tlemma: mind\n",
      "word: how \tlemma: how\n",
      "word: long \tlemma: long\n",
      "word: precisely \tlemma: precisely\n",
      "word: — \tlemma: —\n",
      "word: having \tlemma: have\n",
      "word: little \tlemma: little\n",
      "word: or \tlemma: or\n",
      "word: no \tlemma: no\n",
      "word: money \tlemma: money\n",
      "word: in \tlemma: in\n",
      "word: my \tlemma: my\n",
      "word: purse \tlemma: purse\n",
      "word: , \tlemma: ,\n",
      "word: and \tlemma: and\n",
      "word: nothing \tlemma: nothing\n",
      "word: particular \tlemma: particular\n",
      "word: to \tlemma: to\n",
      "word: interest \tlemma: interest\n",
      "word: me \tlemma: I\n",
      "word: on \tlemma: on\n",
      "word: shore \tlemma: shore\n",
      "word: , \tlemma: ,\n",
      "word: I \tlemma: I\n",
      "word: thought \tlemma: think\n",
      "word: I \tlemma: I\n",
      "word: would \tlemma: would\n",
      "word: sail \tlemma: sail\n",
      "word: about \tlemma: about\n",
      "word: a \tlemma: a\n",
      "word: little \tlemma: little\n",
      "word: and \tlemma: and\n",
      "word: see \tlemma: see\n",
      "word: the \tlemma: the\n",
      "word: watery \tlemma: watery\n",
      "word: part \tlemma: part\n",
      "word: of \tlemma: of\n",
      "word: the \tlemma: the\n",
      "word: world \tlemma: world\n",
      "word: . \tlemma: .\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize, lemma', download_method=None) \n",
    "\n",
    "doc = nlp(\"\"\"Call me Ishmael. Some years ago—never mind how long precisely—having little or no money in my purse, and nothing particular to interest me on shore, I thought I would sail about a little and see the watery part of the world.\"\"\")\n",
    "\n",
    "print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  6. Part-of-Speech & Morphologische Features\n",
    "\n",
    "> The Part-of-Speech (POS) & morphological features tagging module labels words with their universal POS (UPOS) tags, treebank-specific POS (XPOS) tags, and universal morphological features (UFeats).<br>https://stanfordnlp.github.io/stanza/pos.html\n",
    "\n",
    "### *Requirements:* `tokenize, mwt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 07:30:40 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "========================\n",
      "\n",
      "2022-01-21 07:30:40 INFO: Use device: cpu\n",
      "2022-01-21 07:30:40 INFO: Loading: tokenize\n",
      "2022-01-21 07:30:40 INFO: Loading: pos\n",
      "2022-01-21 07:30:41 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: Call\tupos: VERB\txpos: VB\tfeats: Mood=Imp|VerbForm=Fin\n",
      "word: me\tupos: PRON\txpos: PRP\tfeats: Case=Acc|Number=Sing|Person=1|PronType=Prs\n",
      "word: Ishmael\tupos: PROPN\txpos: NNP\tfeats: Number=Sing\n",
      "word: .\tupos: PUNCT\txpos: .\tfeats: _\n",
      "word: Some\tupos: DET\txpos: DT\tfeats: _\n",
      "word: years\tupos: NOUN\txpos: NNS\tfeats: Number=Plur\n",
      "word: ago\tupos: ADV\txpos: RB\tfeats: Degree=Pos\n",
      "word: —\tupos: PUNCT\txpos: :\tfeats: _\n",
      "word: never\tupos: ADV\txpos: RB\tfeats: _\n",
      "word: mind\tupos: VERB\txpos: VB\tfeats: Mood=Imp|Person=2|VerbForm=Fin\n",
      "word: how\tupos: SCONJ\txpos: WRB\tfeats: PronType=Int\n",
      "word: long\tupos: ADV\txpos: RB\tfeats: Degree=Pos\n",
      "word: precisely\tupos: ADV\txpos: RB\tfeats: Degree=Pos\n",
      "word: —\tupos: PUNCT\txpos: :\tfeats: _\n",
      "word: having\tupos: VERB\txpos: VBG\tfeats: VerbForm=Ger\n",
      "word: little\tupos: ADJ\txpos: JJ\tfeats: Degree=Pos\n",
      "word: or\tupos: CCONJ\txpos: CC\tfeats: _\n",
      "word: no\tupos: DET\txpos: DT\tfeats: Polarity=Neg\n",
      "word: money\tupos: NOUN\txpos: NN\tfeats: Number=Sing\n",
      "word: in\tupos: ADP\txpos: IN\tfeats: _\n",
      "word: my\tupos: PRON\txpos: PRP$\tfeats: Number=Sing|Person=1|Poss=Yes|PronType=Prs\n",
      "word: purse\tupos: NOUN\txpos: NN\tfeats: Number=Sing\n",
      "word: ,\tupos: PUNCT\txpos: ,\tfeats: _\n",
      "word: and\tupos: CCONJ\txpos: CC\tfeats: _\n",
      "word: nothing\tupos: PRON\txpos: NN\tfeats: Number=Sing\n",
      "word: particular\tupos: ADJ\txpos: JJ\tfeats: Degree=Pos\n",
      "word: to\tupos: PART\txpos: TO\tfeats: _\n",
      "word: interest\tupos: VERB\txpos: VB\tfeats: VerbForm=Inf\n",
      "word: me\tupos: PRON\txpos: PRP\tfeats: Case=Acc|Number=Sing|Person=1|PronType=Prs\n",
      "word: on\tupos: ADP\txpos: IN\tfeats: _\n",
      "word: shore\tupos: NOUN\txpos: NN\tfeats: Number=Sing\n",
      "word: ,\tupos: PUNCT\txpos: ,\tfeats: _\n",
      "word: I\tupos: PRON\txpos: PRP\tfeats: Case=Nom|Number=Sing|Person=1|PronType=Prs\n",
      "word: thought\tupos: VERB\txpos: VBD\tfeats: Mood=Ind|Number=Sing|Person=1|Tense=Past|VerbForm=Fin\n",
      "word: I\tupos: PRON\txpos: PRP\tfeats: Case=Nom|Number=Sing|Person=1|PronType=Prs\n",
      "word: would\tupos: AUX\txpos: MD\tfeats: VerbForm=Fin\n",
      "word: sail\tupos: VERB\txpos: VB\tfeats: VerbForm=Inf\n",
      "word: about\tupos: ADP\txpos: IN\tfeats: _\n",
      "word: a\tupos: DET\txpos: DT\tfeats: Definite=Ind|PronType=Art\n",
      "word: little\tupos: ADJ\txpos: JJ\tfeats: Degree=Pos\n",
      "word: and\tupos: CCONJ\txpos: CC\tfeats: _\n",
      "word: see\tupos: VERB\txpos: VB\tfeats: VerbForm=Inf\n",
      "word: the\tupos: DET\txpos: DT\tfeats: Definite=Def|PronType=Art\n",
      "word: watery\tupos: ADJ\txpos: JJ\tfeats: Degree=Pos\n",
      "word: part\tupos: NOUN\txpos: NN\tfeats: Number=Sing\n",
      "word: of\tupos: ADP\txpos: IN\tfeats: _\n",
      "word: the\tupos: DET\txpos: DT\tfeats: Definite=Def|PronType=Art\n",
      "word: world\tupos: NOUN\txpos: NN\tfeats: Number=Sing\n",
      "word: .\tupos: PUNCT\txpos: .\tfeats: _\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,pos', download_method=None)\n",
    "doc = nlp(\"\"\"Call me Ishmael. Some years ago—never mind how long precisely—having little or no money in my purse, and nothing particular to interest me on shore, I thought I would sail about a little and see the watery part of the world.\"\"\")\n",
    "\n",
    "print(*[f'word: {word.text}\\tupos: {word.upos}\\txpos: {word.xpos}\\tfeats: {word.feats if word.feats else \"_\"}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Dependency Parsing\n",
    "\n",
    "> The dependency parsing module builds a tree structure of words from the input sentence, which represents the syntactic dependency relations between words. The resulting tree representations, which follow the Universal Dependencies formalism, are useful in many downstream applications.<br>https://stanfordnlp.github.io/stanza/depparse.html\n",
    "\n",
    "\n",
    "### *Requirements:* `tokenize, mwt, pos, lemma`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 07:30:41 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "| depparse  | combined |\n",
      "========================\n",
      "\n",
      "2022-01-21 07:30:41 INFO: Use device: cpu\n",
      "2022-01-21 07:30:41 INFO: Loading: tokenize\n",
      "2022-01-21 07:30:41 INFO: Loading: pos\n",
      "2022-01-21 07:30:41 INFO: Loading: lemma\n",
      "2022-01-21 07:30:41 INFO: Loading: depparse\n",
      "2022-01-21 07:30:42 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 1\tword: Chris\thead id: 3\thead: teaches\tdeprel: nsubj\n",
      "id: 2\tword: Manning\thead id: 1\thead: Chris\tdeprel: flat\n",
      "id: 3\tword: teaches\thead id: 0\thead: root\tdeprel: root\n",
      "id: 4\tword: at\thead id: 6\thead: University\tdeprel: case\n",
      "id: 5\tword: Stanford\thead id: 6\thead: University\tdeprel: compound\n",
      "id: 6\tword: University\thead id: 3\thead: teaches\tdeprel: obl\n",
      "id: 7\tword: .\thead id: 3\thead: teaches\tdeprel: punct\n",
      "id: 1\tword: He\thead id: 2\thead: lives\tdeprel: nsubj\n",
      "id: 2\tword: lives\thead id: 0\thead: root\tdeprel: root\n",
      "id: 3\tword: in\thead id: 6\thead: Area\tdeprel: case\n",
      "id: 4\tword: the\thead id: 6\thead: Area\tdeprel: det\n",
      "id: 5\tword: Bay\thead id: 6\thead: Area\tdeprel: compound\n",
      "id: 6\tword: Area\thead id: 2\thead: lives\tdeprel: obl\n",
      "id: 7\tword: .\thead id: 2\thead: lives\tdeprel: punct\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,lemma,depparse', download_method=None)\n",
    "doc = nlp('Chris Manning teaches at Stanford University. He lives in the Bay Area.')\n",
    "\n",
    "print(*[f'id: {word.id}\\tword: {word.text}\\thead id: {word.head}\\thead: {sent.words[word.head-1].text if word.head > 0 else \"root\"}\\tdeprel: {word.deprel}' for sent in doc.sentences for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  8. Sentiment Analysis\n",
    "\n",
    "https://stanfordnlp.github.io/stanza/sentiment.html\n",
    "\n",
    "### *Requirements:* `tokenize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 07:30:44 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| sentiment | sstplus  |\n",
      "========================\n",
      "\n",
      "2022-01-21 07:30:44 INFO: Use device: cpu\n",
      "2022-01-21 07:30:44 INFO: Loading: tokenize\n",
      "2022-01-21 07:30:44 INFO: Loading: sentiment\n",
      "2022-01-21 07:30:45 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence 0 : sentiment: 1\n",
      "sentence 1 : sentiment: 0\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,sentiment', download_method=None)\n",
    "doc = nlp('I like. I hate.')\n",
    "\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print('sentence',i, ': sentiment:', sentence.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Named Entity Recognition (NER)\n",
    "\n",
    "> The named entity recognition (NER) module recognizes mention spans of a particular entity type (e.g., Person or Organization) in the input sentence. NER is widely used in many NLP applications such as information extraction or question answering systems.<br>https://stanfordnlp.github.io/stanza/ner.html\n",
    "\n",
    "### *Requirements:* `tokenize, mwt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-21 07:30:45 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2022-01-21 07:30:45 INFO: Use device: cpu\n",
      "2022-01-21 07:30:45 INFO: Loading: tokenize\n",
      "2022-01-21 07:30:45 INFO: Loading: ner\n",
      "2022-01-21 07:30:46 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity: Chris Manning\ttype: PERSON\n",
      "entity: Stanford University\ttype: ORG\n",
      "entity: the Bay Area\ttype: LOC\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,ner', download_method=None)\n",
    "doc = nlp(\"Chris Manning teaches at Stanford University. He lives in the Bay Area.\")\n",
    "\n",
    "# sentence based NER output\n",
    "print(*[f'entity: {ent.text}\\ttype: {ent.type}' for ent in doc.ents], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token: Chris\tner: B-PERSON\n",
      "token: Manning\tner: E-PERSON\n",
      "token: teaches\tner: O\n",
      "token: at\tner: O\n",
      "token: Stanford\tner: B-ORG\n",
      "token: University\tner: E-ORG\n",
      "token: .\tner: O\n",
      "token: He\tner: O\n",
      "token: lives\tner: O\n",
      "token: in\tner: O\n",
      "token: the\tner: B-LOC\n",
      "token: Bay\tner: I-LOC\n",
      "token: Area\tner: E-LOC\n",
      "token: .\tner: O\n"
     ]
    }
   ],
   "source": [
    "# token based NER output\n",
    "print(*[f'token: {token.text}\\tner: {token.ner}' for sent in doc.sentences for token in sent.tokens], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\n",
      "  \"text\": \"Chris Manning\",\n",
      "  \"type\": \"PERSON\",\n",
      "  \"start_char\": 0,\n",
      "  \"end_char\": 13\n",
      "}, {\n",
      "  \"text\": \"Stanford University\",\n",
      "  \"type\": \"ORG\",\n",
      "  \"start_char\": 25,\n",
      "  \"end_char\": 44\n",
      "}, {\n",
      "  \"text\": \"the Bay Area\",\n",
      "  \"type\": \"LOC\",\n",
      "  \"start_char\": 58,\n",
      "  \"end_char\": 70\n",
      "}]\n"
     ]
    }
   ],
   "source": [
    "print(doc.entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER-Visualisierung mit spaCy\n",
    "\n",
    "- spaCy ist eine open-source Python NLP Bibliothek, im Funktionsumfang vergleichbar mit stanza\n",
    "- spaCy bietet einen Feature Visaulisierer für Parsebäume und NERs an (displacy)\n",
    "\n",
    "https://spacy.io/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chris Manning PERSON\n",
      "Stanford University ORG\n",
      "the Bay Area LOC\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"Chris Manning teaches at Stanford University. He lives in the Bay Area.\")\n",
    "\n",
    "for word in doc.ents:\n",
    "    print(word.text,word.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"01085ec464d244189be9bede5e6a247d-0\" class=\"displacy\" width=\"1010\" height=\"257.0\" direction=\"ltr\" style=\"max-width: none; height: 257.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Chris</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"130\">Manning</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"130\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"210\">teaches</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"210\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"290\">at</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"290\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"370\">Stanford</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"370\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"450\">University.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"450\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"530\">He</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"530\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"610\">lives</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"610\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"690\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"690\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"850\">Bay</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"850\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"167.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"930\">Area.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"930\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-01085ec464d244189be9bede5e6a247d-0-0\" stroke-width=\"2px\" d=\"M70,122.0 C70,82.0 120.0,82.0 120.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-01085ec464d244189be9bede5e6a247d-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,124.0 L62,112.0 78,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-01085ec464d244189be9bede5e6a247d-0-1\" stroke-width=\"2px\" d=\"M150,122.0 C150,82.0 200.0,82.0 200.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-01085ec464d244189be9bede5e6a247d-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M150,124.0 L142,112.0 158,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-01085ec464d244189be9bede5e6a247d-0-2\" stroke-width=\"2px\" d=\"M230,122.0 C230,82.0 280.0,82.0 280.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-01085ec464d244189be9bede5e6a247d-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M280.0,124.0 L288.0,112.0 272.0,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-01085ec464d244189be9bede5e6a247d-0-3\" stroke-width=\"2px\" d=\"M390,122.0 C390,82.0 440.0,82.0 440.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-01085ec464d244189be9bede5e6a247d-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M390,124.0 L382,112.0 398,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-01085ec464d244189be9bede5e6a247d-0-4\" stroke-width=\"2px\" d=\"M310,122.0 C310,42.0 445.0,42.0 445.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-01085ec464d244189be9bede5e6a247d-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M445.0,124.0 L453.0,112.0 437.0,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-01085ec464d244189be9bede5e6a247d-0-5\" stroke-width=\"2px\" d=\"M550,122.0 C550,82.0 600.0,82.0 600.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-01085ec464d244189be9bede5e6a247d-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M550,124.0 L542,112.0 558,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-01085ec464d244189be9bede5e6a247d-0-6\" stroke-width=\"2px\" d=\"M630,122.0 C630,82.0 680.0,82.0 680.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-01085ec464d244189be9bede5e6a247d-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M680.0,124.0 L688.0,112.0 672.0,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-01085ec464d244189be9bede5e6a247d-0-7\" stroke-width=\"2px\" d=\"M790,122.0 C790,42.0 925.0,42.0 925.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-01085ec464d244189be9bede5e6a247d-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M790,124.0 L782,112.0 798,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-01085ec464d244189be9bede5e6a247d-0-8\" stroke-width=\"2px\" d=\"M870,122.0 C870,82.0 920.0,82.0 920.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-01085ec464d244189be9bede5e6a247d-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M870,124.0 L862,112.0 878,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-01085ec464d244189be9bede5e6a247d-0-9\" stroke-width=\"2px\" d=\"M710,122.0 C710,2.0 930.0,2.0 930.0,122.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-01085ec464d244189be9bede5e6a247d-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M930.0,124.0 L938.0,112.0 922.0,112.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"dep\", options={'distance':80})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Chris Manning\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " teaches at \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Stanford University\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ". He lives in \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the Bay Area\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Daten-Konvertierungen\n",
    "\n",
    "https://stanfordnlp.github.io/stanza/data_conversion.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-30 10:55:46 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "| depparse  | combined |\n",
      "========================\n",
      "\n",
      "2022-05-30 10:55:46 INFO: Use device: cpu\n",
      "2022-05-30 10:55:46 INFO: Loading: tokenize\n",
      "2022-05-30 10:55:46 INFO: Loading: pos\n",
      "2022-05-30 10:55:46 INFO: Loading: lemma\n",
      "2022-05-30 10:55:46 INFO: Loading: depparse\n",
      "2022-05-30 10:55:47 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,pos,lemma,depparse', download_method=None)\n",
    "doc = nlp('He lives in Munich.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stanza.models.common.doc.Document"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stanza.models.common.doc.Word"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc.sentences[0].words[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"id\": 1,\n",
       "  \"text\": \"He\",\n",
       "  \"lemma\": \"he\",\n",
       "  \"upos\": \"PRON\",\n",
       "  \"xpos\": \"PRP\",\n",
       "  \"feats\": \"Case=Nom|Gender=Masc|Number=Sing|Person=3|PronType=Prs\",\n",
       "  \"head\": 2,\n",
       "  \"deprel\": \"nsubj\",\n",
       "  \"start_char\": 0,\n",
       "  \"end_char\": 2\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.sentences[0].words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He lives in Munich .\n"
     ]
    }
   ],
   "source": [
    "print(*[f'{word.text}' for sent in doc.sentences for word in sent.words], sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "  [\n",
       "    {\n",
       "      \"id\": 1,\n",
       "      \"text\": \"He\",\n",
       "      \"lemma\": \"he\",\n",
       "      \"upos\": \"PRON\",\n",
       "      \"xpos\": \"PRP\",\n",
       "      \"feats\": \"Case=Nom|Gender=Masc|Number=Sing|Person=3|PronType=Prs\",\n",
       "      \"head\": 2,\n",
       "      \"deprel\": \"nsubj\",\n",
       "      \"start_char\": 0,\n",
       "      \"end_char\": 2\n",
       "    },\n",
       "    {\n",
       "      \"id\": 2,\n",
       "      \"text\": \"lives\",\n",
       "      \"lemma\": \"live\",\n",
       "      \"upos\": \"VERB\",\n",
       "      \"xpos\": \"VBZ\",\n",
       "      \"feats\": \"Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\",\n",
       "      \"head\": 0,\n",
       "      \"deprel\": \"root\",\n",
       "      \"start_char\": 3,\n",
       "      \"end_char\": 8\n",
       "    },\n",
       "    {\n",
       "      \"id\": 3,\n",
       "      \"text\": \"in\",\n",
       "      \"lemma\": \"in\",\n",
       "      \"upos\": \"ADP\",\n",
       "      \"xpos\": \"IN\",\n",
       "      \"head\": 4,\n",
       "      \"deprel\": \"case\",\n",
       "      \"start_char\": 9,\n",
       "      \"end_char\": 11\n",
       "    },\n",
       "    {\n",
       "      \"id\": 4,\n",
       "      \"text\": \"Munich\",\n",
       "      \"lemma\": \"Munich\",\n",
       "      \"upos\": \"PROPN\",\n",
       "      \"xpos\": \"NNP\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 2,\n",
       "      \"deprel\": \"obl\",\n",
       "      \"start_char\": 12,\n",
       "      \"end_char\": 18\n",
       "    },\n",
       "    {\n",
       "      \"id\": 5,\n",
       "      \"text\": \".\",\n",
       "      \"lemma\": \".\",\n",
       "      \"upos\": \"PUNCT\",\n",
       "      \"xpos\": \".\",\n",
       "      \"head\": 2,\n",
       "      \"deprel\": \"punct\",\n",
       "      \"start_char\": 18,\n",
       "      \"end_char\": 19\n",
       "    }\n",
       "  ]\n",
       "]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Object in Python Datenstruktur transformieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicts = doc.to_dict() # dicts is List[List[Dict]], representing each token / word in each sentence in the document\n",
    "type(dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He lives in Munich .\n"
     ]
    }
   ],
   "source": [
    "print(*[f'{word[\"text\"]}' for sent in dicts for word in sent], sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'deprel': 'nsubj',\n",
       " 'end_char': 2,\n",
       " 'feats': 'Case=Nom|Gender=Masc|Number=Sing|Person=3|PronType=Prs',\n",
       " 'head': 2,\n",
       " 'id': 1,\n",
       " 'lemma': 'he',\n",
       " 'start_char': 0,\n",
       " 'text': 'He',\n",
       " 'upos': 'PRON',\n",
       " 'xpos': 'PRP'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicts[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'deprel': 'nsubj',\n",
       "   'end_char': 2,\n",
       "   'feats': 'Case=Nom|Gender=Masc|Number=Sing|Person=3|PronType=Prs',\n",
       "   'head': 2,\n",
       "   'id': 1,\n",
       "   'lemma': 'he',\n",
       "   'start_char': 0,\n",
       "   'text': 'He',\n",
       "   'upos': 'PRON',\n",
       "   'xpos': 'PRP'},\n",
       "  {'deprel': 'root',\n",
       "   'end_char': 8,\n",
       "   'feats': 'Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin',\n",
       "   'head': 0,\n",
       "   'id': 2,\n",
       "   'lemma': 'live',\n",
       "   'start_char': 3,\n",
       "   'text': 'lives',\n",
       "   'upos': 'VERB',\n",
       "   'xpos': 'VBZ'},\n",
       "  {'deprel': 'case',\n",
       "   'end_char': 11,\n",
       "   'head': 4,\n",
       "   'id': 3,\n",
       "   'lemma': 'in',\n",
       "   'start_char': 9,\n",
       "   'text': 'in',\n",
       "   'upos': 'ADP',\n",
       "   'xpos': 'IN'},\n",
       "  {'deprel': 'obl',\n",
       "   'end_char': 18,\n",
       "   'feats': 'Number=Sing',\n",
       "   'head': 2,\n",
       "   'id': 4,\n",
       "   'lemma': 'Munich',\n",
       "   'start_char': 12,\n",
       "   'text': 'Munich',\n",
       "   'upos': 'PROPN',\n",
       "   'xpos': 'NNP'},\n",
       "  {'deprel': 'punct',\n",
       "   'end_char': 19,\n",
       "   'head': 2,\n",
       "   'id': 5,\n",
       "   'lemma': '.',\n",
       "   'start_char': 18,\n",
       "   'text': '.',\n",
       "   'upos': 'PUNCT',\n",
       "   'xpos': '.'}]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Object in CoNLL transformieren \n",
    "\n",
    "- CoNLL ist ein Standardformatn für tabulare Dependency Parsing Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stanza.utils.conll import CoNLL\n",
    "conll = CoNLL.convert_dict(dicts) # conll is List[List[List]], representing each token / word in each sentence in the document\n",
    "type(conll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[['1',\n",
       "   'He',\n",
       "   'he',\n",
       "   'PRON',\n",
       "   'PRP',\n",
       "   'Case=Nom|Gender=Masc|Number=Sing|Person=3|PronType=Prs',\n",
       "   '2',\n",
       "   'nsubj',\n",
       "   '_',\n",
       "   'start_char=0|end_char=2'],\n",
       "  ['2',\n",
       "   'lives',\n",
       "   'live',\n",
       "   'VERB',\n",
       "   'VBZ',\n",
       "   'Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin',\n",
       "   '0',\n",
       "   'root',\n",
       "   '_',\n",
       "   'start_char=3|end_char=8'],\n",
       "  ['3',\n",
       "   'in',\n",
       "   'in',\n",
       "   'ADP',\n",
       "   'IN',\n",
       "   '_',\n",
       "   '4',\n",
       "   'case',\n",
       "   '_',\n",
       "   'start_char=9|end_char=11'],\n",
       "  ['4',\n",
       "   'Munich',\n",
       "   'Munich',\n",
       "   'PROPN',\n",
       "   'NNP',\n",
       "   'Number=Sing',\n",
       "   '2',\n",
       "   'obl',\n",
       "   '_',\n",
       "   'start_char=12|end_char=18'],\n",
       "  ['5',\n",
       "   '.',\n",
       "   '.',\n",
       "   'PUNCT',\n",
       "   '.',\n",
       "   '_',\n",
       "   '2',\n",
       "   'punct',\n",
       "   '_',\n",
       "   'start_char=18|end_char=19']]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# &Uuml;bungsaufgaben 6\n",
    "\n",
    "\n",
    "## Aufgabe 1 (eigenen Sentence-Segmenter erstellen)\n",
    "\n",
    "Satzsegmentierung (End-of-Sentence-Detection) kann als binäre Klassifikation verstanden werden (s. https://www.nltk.org/book/ch06.html#sentence-segmentation), die für jedes Token in einem Korpus entscheidet, ob es ein ***sentence boundary token*** ist oder nicht. Dies ist genauer eine **Sequenzklassifikation**, da die Entscheidung abhängt vom ***Kontext der Punktuationszeichen*** (z.B. `['Mr', '.']`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK besitzt mit `sent_tokenize` eine Methode zur Satzsegmentierung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"You hear that Mr. Anderson? That is the sound of inevitability. Good-bye, Mr. Anderson! END\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You hear that Mr. Anderson?', 'That is the sound of inevitability.', 'Good-bye, Mr. Anderson!', 'END']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "sents = nltk.sent_tokenize(text)\n",
    "print(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Erzeugen Sie einen ***(1) regelbasierten*** sowie einen ***(2) auf Satz-Segmentationsdaten der Penn-Treebank trainierten*** **Punktuationsklassifikator zur Satzsegmentierung**. \n",
    "\n",
    "Input soll eine Wordliste mit einer einfachen Tokenisierung sein, wie in folgendem englischen Beispielsatz, mit dem Sie Ihre Klassifikatoren auch testen sollen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', 'hear', 'that', 'Mr', '.', 'Anderson', '?', 'That', 'is', 'the', 'sound', 'of', 'inevitability', '.', 'Good', '-', 'bye', ',', 'Mr', '.', 'Anderson', '!', 'END']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "test_words = re.findall(r'\\w+|[^\\w\\s]+', text)  #entspricht nltk.wordpunct_tokenize\n",
    "print(test_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 1a (Rule-based Sentence Segmentation)\n",
    "\n",
    "\n",
    "Erstellen Sie einen einfachen regelbasierten Punctuation Tagger, der eine Liste von Wort- und Punktuationstokens in eine Liste von entsprechenden Satz-Tokenlisten auftrennt. Orientieren Sie sich dabei an https://en.wikipedia.org/wiki/Sentence_boundary_disambiguation:\n",
    "\n",
    ">   (a) If it's a period, it ends a sentence.<br>\n",
    "    (b) If the preceding token is in the hand-compiled list of abbreviations, then it doesn't end a sentence.<br>\n",
    "    (c) If the next token is capitalized, then it ends a sentence.        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_sentences_rule_based(words):\n",
    "    sent_list = []\n",
    "    sent = []\n",
    "    for index,token in enumerate(words):\n",
    "        sent.append(token)\n",
    "        if token == 'END': #baseline\n",
    "            sent_list.append(sent)\n",
    "            sent = []\n",
    "    return sent_list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['You',\n",
       "  'hear',\n",
       "  'that',\n",
       "  'Mr',\n",
       "  '.',\n",
       "  'Anderson',\n",
       "  '?',\n",
       "  'That',\n",
       "  'is',\n",
       "  'the',\n",
       "  'sound',\n",
       "  'of',\n",
       "  'inevitability',\n",
       "  '.',\n",
       "  'Good',\n",
       "  'bye',\n",
       "  ',',\n",
       "  'Mr',\n",
       "  '.',\n",
       "  'Anderson',\n",
       "  '!',\n",
       "  'END']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_sentences_rule_based(test_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 1b (Supervised Sentence Segmentation)\n",
    "\n",
    "Trainieren Sie einen Punctuation Classifier mit Hilfe der Daten zur Satzsegmentierung der Penn-Treebank. Orientieren Sie sich dabei am Vorgehen in https://www.nltk.org/book/ch06.html#sentence-segmentation:\n",
    "\n",
    "- extract features for possible sentence-boundary tokens\n",
    "- learn mapping from feature-representations to binary end-of-sentence classes (boundary yes/no)\n",
    "- training data: corpus with annotation of sentence boundaries (e.g. treebanks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 2 (Korpusannotation mit stanza)\n",
    "\n",
    "Annotieren Sie den Text in `wahlverwandschaften.txt` nach morphologischen, syntaktischen und semantischen Kategorien mit Hilfe der deutschen stanza-Modelle.\n",
    "\n",
    "Verwenden Sie dabei auch die CoNLL-Utilities von stanza für eine Transformation eines Dependency-analysierten Satzes in das CoNLL-Format, um es als NLTK-Dependency-Tree-Objekt einzulesen und zu plotten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 3 (Weiterverarbeitung Korpusannotation)\n",
    "\n",
    "Führen Sie auf dem Wahlverwandschaften-Text mit stanza ein POS-Tagging aus und verwenden Sie die Ausgabe für eine POS-Frequenzzählung und Plotting der Ergebnisse mit matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x128d662b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Code: https://www.python-graph-gallery.com/barplot/\n",
    "\n",
    "# Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Make a random dataset:\n",
    "height = [3, 12, 5, 18, 45]\n",
    "bars = ('A', 'B', 'C', 'D', 'E')\n",
    "y_pos = np.arange(len(bars))\n",
    "\n",
    "# Create bars\n",
    "plt.bar(y_pos, height)\n",
    "\n",
    "# Create names on the x-axis\n",
    "plt.xticks(y_pos, bars)\n",
    "\n",
    "# Show graphic\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 4: Tagging mit NLTK\n",
    "\n",
    "Auch NLTK enthält Modelle für Textannotationen. Testen Sie die in den NLTK-Kapiteln 3 und 5 beschriebenen Tagger (**POS, Segmentizer, Stemmer, Lemmatizer**) für das Englische aus (wie man einen POS-Tagger mit NLTK selbst trainiert, um etwa auch auf deutschen Texten POS-Tagging mit NLTK durchzuführen, ist Thema in einer späteren Sitzung).\n",
    "\n",
    "- https://www.nltk.org/book/ch03.html\n",
    "- https://www.nltk.org/book/ch05.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
