{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Übungsaufgaben 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Aufgabe 1 (eigenen Sentence-Segmenter erstellen)\n",
    "\n",
    "Satzsegmentierung (End-of-Sentence-Detection) kann als binäre Klassifikation verstanden werden (s. https://www.nltk.org/book/ch06.html#sentence-segmentation), die für jedes Token in einem Korpus entscheidet, ob es ein ***sentence boundary token*** ist oder nicht. Dies ist genauer eine **Sequenzklassifikation**, da die Entscheidung abhängt vom ***Kontext der Punktuationszeichen*** (z.B. `['Mr', '.']`).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK besitzt mit `sent_tokenize` eine Methode zur Satzsegmentierung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"You hear that Mr. Anderson? That is the sound of inevitability. Good-bye, Mr. Anderson! END\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You hear that Mr. Anderson?', 'That is the sound of inevitability.', 'Good-bye, Mr. Anderson!', 'END']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "sents = nltk.sent_tokenize(text)\n",
    "print(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Erzeugen Sie einen ***(1) regelbasierten*** sowie einen ***(2) auf Satz-Segmentationsdaten der Penn-Treebank trainierten*** **Punktuationsklassifikator zur Satzsegmentierung**. \n",
    "\n",
    "Input soll eine Wordliste mit einer einfachen Tokenisierung sein, wie in folgendem englischen Beispielsatz, mit dem Sie Ihre Klassifikatoren auch testen sollen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', 'hear', 'that', 'Mr', '.', 'Anderson', '?', 'That', 'is', 'the', 'sound', 'of', 'inevitability', '.', 'Good', '-', 'bye', ',', 'Mr', '.', 'Anderson', '!', 'END']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "test_words = re.findall(r'\\w+|[^\\w\\s]+', text)  #entspricht nltk.wordpunct_tokenize\n",
    "print(test_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Erklärung zum wordpunct_tokenize-REGEXP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', 'hear', 'that', 'Mr', 'Anderson', 'That', 'is', 'the', 'sound', 'of', 'inevitability', 'Good', 'bye', 'Mr', 'Anderson', 'END']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(r'\\w+', text)) #findet alle Wörter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', '?', '.', '-', ',', '.', '!']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(r'[^\\w\\s]+', text)) #findet alle Punktuationszeichen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', 'hear', 'that', 'Mr.', 'Anderson?', 'That', 'is', 'the', 'sound', 'of', 'inevitability.', 'Good-bye,', 'Mr.', 'Anderson!', 'END']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(r'[^\\s]+', text)) #\\s matches whitespace characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 1a (Rule-based Sentence Segmentation)\n",
    "\n",
    "\n",
    "Erstellen Sie einen einfachen regelbasierten Punctuation Tagger, der eine Liste von Wort- und Punktuationstokens in eine Liste von entsprechenden Satz-Tokenlisten auftrennt. Orientieren Sie sich dabei an https://en.wikipedia.org/wiki/Sentence_boundary_disambiguation:\n",
    "\n",
    ">   (a) If it's a period, it ends a sentence.<br>\n",
    "    (b) If the preceding token is in the hand-compiled list of abbreviations, then it doesn't end a sentence.<br>\n",
    "    (c) If the next token is capitalized, then it ends a sentence.        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_sentences_rule_based(words):\n",
    "    sent_list = []\n",
    "    sent = []\n",
    "    for index,token in enumerate(words):\n",
    "        sent.append(token)\n",
    "        if token in '.?!' and words[index-1] not in [\"Mr\", \"Mrs\", \"Ms\", \"Dr\"] and words[index+1][0].isupper():\n",
    "            sent_list.append(sent)\n",
    "            sent = []\n",
    "    return sent_list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['You', 'hear', 'that', 'Mr', '.', 'Anderson', '?'],\n",
       " ['That', 'is', 'the', 'sound', 'of', 'inevitability', '.'],\n",
       " ['Good', '-', 'bye', ',', 'Mr', '.', 'Anderson', '!']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_sentences_rule_based(test_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aufgabe 1b (Supervised Sentence Segmentation)\n",
    "\n",
    "Trainieren Sie einen Punctuation Classifier mit Hilfe der Daten zur Satzsegmentierung der Penn-Treebank. Orientieren Sie sich dabei am Vorgehen in https://www.nltk.org/book/ch06.html#sentence-segmentation:\n",
    "\n",
    "- extract features for possible sentence-boundary tokens\n",
    "- learn mapping from feature-representations to binary end-of-sentence classes (boundary yes/no) \n",
    "- training data: corpus with annotation of sentence boundaries (e.g. treebanks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import treebank # Sample of Penn Treebank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use Penn Treebank as Training Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The first step is to obtain some data that has already been segmented into sentences \n",
    "#and convert it into a form that is suitable for extracting features:\n",
    "sents = nltk.corpus.treebank_raw.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['.', 'START'], ['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'Nov', '.', '29', '.'], ['Mr', '.', 'Vinken', 'is', 'chairman', 'of', 'Elsevier', 'N', '.', 'V', '.,', 'the', 'Dutch', 'publishing', 'group', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(sents[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here, tokens is a merged list of tokens from the individual sentences, \n",
    "#and boundaries is a set containing the indexes of all sentence-boundary tokens. \n",
    "tokens = []\n",
    "boundaries = set()\n",
    "offset = 0\n",
    "for sent in sents:\n",
    "    tokens.extend(sent)\n",
    "    offset += len(sent)\n",
    "    boundaries.add(offset-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '.'),\n",
       " (1, 'START'),\n",
       " (2, 'Pierre'),\n",
       " (3, 'Vinken'),\n",
       " (4, ','),\n",
       " (5, '61'),\n",
       " (6, 'years'),\n",
       " (7, 'old'),\n",
       " (8, ','),\n",
       " (9, 'will'),\n",
       " (10, 'join'),\n",
       " (11, 'the'),\n",
       " (12, 'board'),\n",
       " (13, 'as'),\n",
       " (14, 'a'),\n",
       " (15, 'nonexecutive'),\n",
       " (16, 'director'),\n",
       " (17, 'Nov'),\n",
       " (18, '.'),\n",
       " (19, '29'),\n",
       " (20, '.'),\n",
       " (21, 'Mr'),\n",
       " (22, '.'),\n",
       " (23, 'Vinken'),\n",
       " (24, 'is'),\n",
       " (25, 'chairman'),\n",
       " (26, 'of'),\n",
       " (27, 'Elsevier'),\n",
       " (28, 'N'),\n",
       " (29, '.'),\n",
       " (30, 'V'),\n",
       " (31, '.,'),\n",
       " (32, 'the'),\n",
       " (33, 'Dutch'),\n",
       " (34, 'publishing'),\n",
       " (35, 'group'),\n",
       " (36, '.'),\n",
       " (37, '.'),\n",
       " (38, 'START'),\n",
       " (39, 'Rudolph')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(index, token) for index,token in enumerate(tokens[0:40])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 20, 36, 38]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(boundaries))[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Feature-Extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, we need to specify the features of the data that will be used \n",
    "#in order to decide whether punctuation indicates a sentence-boundary:\n",
    "def punct_features(tokens, i):\n",
    "    return {'next-word-capitalized': tokens[i+1][0].isupper(),\n",
    "            'prev-word': tokens[i-1].lower(),\n",
    "            'punct': tokens[i],\n",
    "            'prev-word-is-one-char': len(tokens[i-1]) == 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Based on this feature extractor, we can create a list of labeled featuresets \n",
    "#by selecting all the punctuation tokens, and tagging whether they are boundary tokens or not:\n",
    "featuresets = [(punct_features(tokens, i), (i in boundaries))\n",
    "    for i in range(1, len(tokens)-1)\n",
    "    if tokens[i] in '.?!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'next-word-capitalized': False,\n",
       "   'prev-word': 'nov',\n",
       "   'punct': '.',\n",
       "   'prev-word-is-one-char': False},\n",
       "  False),\n",
       " ({'next-word-capitalized': True,\n",
       "   'prev-word': '29',\n",
       "   'punct': '.',\n",
       "   'prev-word-is-one-char': False},\n",
       "  True),\n",
       " ({'next-word-capitalized': True,\n",
       "   'prev-word': 'mr',\n",
       "   'punct': '.',\n",
       "   'prev-word-is-one-char': False},\n",
       "  False),\n",
       " ({'next-word-capitalized': True,\n",
       "   'prev-word': 'n',\n",
       "   'punct': '.',\n",
       "   'prev-word-is-one-char': True},\n",
       "  False),\n",
       " ({'next-word-capitalized': False,\n",
       "   'prev-word': 'group',\n",
       "   'punct': '.',\n",
       "   'prev-word-is-one-char': False},\n",
       "  True)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresets[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### Train Simple Probabilistic Classifier (Naive Bayes):\n",
    "\n",
    "https://en.wikipedia.org/wiki/Naive_Bayes_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9461279461279462"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using these featuresets, we can train and evaluate a punctuation classifier:\n",
    "size = int(len(featuresets) * 0.1)\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "nltk.classify.accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Natural Language Toolkit: code_classification_based_segmenter\n",
    "\n",
    "def segment_sentences(words):\n",
    "    start = 0\n",
    "    sents = []\n",
    "    for i, word in enumerate(words):\n",
    "        if word in '.?!' and classifier.classify(punct_features(words, i)) == True:\n",
    "            sents.append(words[start:i+1])\n",
    "            start = i+1\n",
    "    if start < len(words):\n",
    "        sents.append(words[start:])\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', 'hear', 'that', 'Mr', '.', 'Anderson', '?', 'That', 'is', 'the', 'sound', 'of', 'inevitability', '.', 'Good', '-', 'bye', ',', 'Mr', '.', 'Anderson', '!', 'END']\n"
     ]
    }
   ],
   "source": [
    "print(test_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['You', 'hear', 'that', 'Mr', '.', 'Anderson', '?'],\n",
       " ['That', 'is', 'the', 'sound', 'of', 'inevitability', '.'],\n",
       " ['Good', '-', 'bye', ',', 'Mr', '.', 'Anderson', '!'],\n",
       " ['END']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment_sentences(test_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 2 (Korpusannotation mit stanza)\n",
    "\n",
    "Annotieren Sie den Text in `wahlverwandschaften.txt` nach morphologischen, syntaktischen und semantischen Kategorien mit Hilfe der deutschen stanza-Modelle.\n",
    "\n",
    "Verwenden Sie dabei auch die CoNLL-Utilities von stanza für eine Transformation eines Dependency-analysierten Satzes in das CoNLL-Format, um es als NLTK-Dependency-Tree-Objekt einzulesen und zu plotten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "#stanza.download('de')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2a: morphologische Analyse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-13 11:41:28 INFO: Loading these models for language: de (German):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "=======================\n",
      "\n",
      "2023-06-13 11:41:28 INFO: Using device: cpu\n",
      "2023-06-13 11:41:28 INFO: Loading: tokenize\n",
      "2023-06-13 11:41:28 INFO: Loading: mwt\n",
      "2023-06-13 11:41:28 INFO: Loading: pos\n",
      "2023-06-13 11:41:28 INFO: Loading: lemma\n",
      "2023-06-13 11:41:28 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "text = open('../wahlverwandschaften.txt').read()\n",
    "nlp = stanza.Pipeline(lang='de', processors='tokenize, mwt, lemma, pos', download_method=None)\n",
    "#doc = nlp(text)#[0:1000])\n",
    "doc = nlp(text[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\n",
       "   {\n",
       "     \"id\": 1,\n",
       "     \"text\": \"Die\",\n",
       "     \"lemma\": \"der\",\n",
       "     \"upos\": \"DET\",\n",
       "     \"xpos\": \"ART\",\n",
       "     \"feats\": \"Case=Nom|Definite=Def|Gender=Fem|Number=Plur|PronType=Art\",\n",
       "     \"start_char\": 0,\n",
       "     \"end_char\": 3\n",
       "   },\n",
       "   {\n",
       "     \"id\": 2,\n",
       "     \"text\": \"Wahlverwandtschaften\",\n",
       "     \"lemma\": \"Wahlverwandtschaft\",\n",
       "     \"upos\": \"NOUN\",\n",
       "     \"xpos\": \"NN\",\n",
       "     \"feats\": \"Case=Nom|Gender=Fem|Number=Plur\",\n",
       "     \"start_char\": 4,\n",
       "     \"end_char\": 24\n",
       "   }\n",
       " ]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.sentences[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sein Geschäft war eben vollendet ;\n"
     ]
    }
   ],
   "source": [
    "print(*[f'{word.text}' for sent in doc.sentences[6:7] for word in sent.words], sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: Sein \tlemma: sein\tupos: DET\txpos: PPOSAT\tfeats: Case=Nom|Gender=Neut|Gender[psor]=Masc,Neut|Number=Sing|Number[psor]=Sing|Person=3|Poss=Yes|PronType=Prs\n",
      "word: Geschäft \tlemma: Geschäft\tupos: NOUN\txpos: NN\tfeats: Case=Nom|Gender=Neut|Number=Sing\n",
      "word: war \tlemma: sein\tupos: AUX\txpos: VAFIN\tfeats: Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\n",
      "word: eben \tlemma: eben\tupos: ADV\txpos: ADV\tfeats: _\n",
      "word: vollendet \tlemma: vollenden\tupos: ADJ\txpos: VVPP\tfeats: Degree=Pos|VerbForm=Part\n",
      "word: ; \tlemma: ;\tupos: PUNCT\txpos: $.\tfeats: _\n"
     ]
    }
   ],
   "source": [
    "print(*[f'word: {word.text+\" \"}\\tlemma: {word.lemma}\\tupos: {word.upos}\\txpos: {word.xpos}\\tfeats: {word.feats if word.feats else \"_\"}' for sent in doc.sentences[6:7] for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b: Sentiment Analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-13 11:41:33 WARNING: Language de package default expects mwt, which has been added\n",
      "2023-06-13 11:41:33 INFO: Loading these models for language: de (German):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "| sentiment | sb10k   |\n",
      "=======================\n",
      "\n",
      "2023-06-13 11:41:33 INFO: Using device: cpu\n",
      "2023-06-13 11:41:33 INFO: Loading: tokenize\n",
      "2023-06-13 11:41:33 INFO: Loading: mwt\n",
      "2023-06-13 11:41:33 INFO: Loading: sentiment\n",
      "2023-06-13 11:41:34 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> sentence 0 : sentiment: 2\n",
      "Die Wahlverwandtschaften\n",
      "=> sentence 1 : sentiment: 2\n",
      "Ein Roman\n",
      "=> sentence 2 : sentiment: 2\n",
      "von Johann Wolfgang von Goethe\n",
      "=> sentence 3 : sentiment: 2\n",
      "Erster Teil\n",
      "=> sentence 4 : sentiment: 2\n",
      "Erstes Kapitel\n",
      "=> sentence 5 : sentiment: 1\n",
      "Eduard—so nennen wir einen reichen Baron in dem besten Mannesalter —Eduard hatte in seiner Baumschule die schönste Stunde eines Aprilnachmittags zugebracht , um frisch erhaltene Pfropfreiser auf junge Stämme zu bringen .\n",
      "=> sentence 6 : sentiment: 2\n",
      "Sein Geschäft war eben vollendet ;\n",
      "=> sentence 7 : sentiment: 1\n",
      "er legte die Gerätschaften in das Futteral zusammen und betrachtete seine Arbeit mit Vergnügen , als der Gärtner hinzutrat und sich an dem teilnehmenden Fleiße des Herrn ergetzte .\n",
      "=> sentence 8 : sentiment: 0\n",
      "„ Hast du meine Frau nicht gesehen ? “ fragte Eduard , indem er sich weiterzugehen anschickte .\n",
      "=> sentence 9 : sentiment: 1\n",
      "„ Drüben in den neuen Anlagen “ , versetzte der Gärtner .\n",
      "=> sentence 10 : sentiment: 1\n",
      "„ Die Mooshütte wird heute fertig , die sie an der Felswand , dem Schlosse gegenüber , gebaut hat .\n",
      "=> sentence 11 : sentiment: 1\n",
      "Alles ist recht schön geworden und muß Euer Gnaden gefallen .\n",
      "=> sentence 12 : sentiment: 1\n",
      "Man hat einen vortrefflichen Anblick : unten das Dorf , ein wenig rechter Hand die Kirche , über deren Turmspitze man fast hinwegsieht , gegenüber das Schloß und die Gär\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='de', processors='tokenize,sentiment', download_method=None)\n",
    "doc = nlp(text[0:1000])\n",
    "\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print('=> sentence',i, ': sentiment:', sentence.sentiment)\n",
    "    print(*[f'{word.text}' for word in sentence.words], sep=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2c: NER:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-13 11:41:40 WARNING: Language de package default expects mwt, which has been added\n",
      "2023-06-13 11:41:40 INFO: Loading these models for language: de (German):\n",
      "============================\n",
      "| Processor | Package      |\n",
      "----------------------------\n",
      "| tokenize  | gsd          |\n",
      "| mwt       | gsd          |\n",
      "| ner       | germeval2014 |\n",
      "============================\n",
      "\n",
      "2023-06-13 11:41:40 INFO: Using device: cpu\n",
      "2023-06-13 11:41:40 INFO: Loading: tokenize\n",
      "2023-06-13 11:41:40 INFO: Loading: mwt\n",
      "2023-06-13 11:41:40 INFO: Loading: ner\n",
      "2023-06-13 11:41:42 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity: Johann Wolfgang von Goethe\ttype: PER\n",
      "entity: Eduard—so\ttype: PER\n",
      "entity: —Eduard\ttype: PER\n",
      "entity: Eduard\ttype: PER\n",
      "entity: Mooshütte\ttype: LOC\n",
      "entity: Eduard\ttype: PER\n",
      "entity: Eduard\ttype: PER\n",
      "entity: Eduard\ttype: PER\n",
      "entity: Mooshütte\ttype: LOC\n",
      "entity: Charlotte\ttype: PER\n",
      "entity: de\ttype: PER\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='de', processors='tokenize,ner', download_method=None)\n",
    "doc = nlp(text[0:2500])\n",
    "\n",
    "# sentence based NER output\n",
    "print(*[f'entity: {ent.text}\\ttype: {ent.type}' for ent in doc.ents], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2d: Dependencies:\n",
    "\n",
    "##### Inklusive Transformation in CONLL-Format für Plotting mit NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-13 11:41:51 WARNING: Language de package default expects mwt, which has been added\n",
      "2023-06-13 11:41:51 INFO: Loading these models for language: de (German):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "| depparse  | gsd     |\n",
      "=======================\n",
      "\n",
      "2023-06-13 11:41:51 INFO: Using device: cpu\n",
      "2023-06-13 11:41:51 INFO: Loading: tokenize\n",
      "2023-06-13 11:41:51 INFO: Loading: mwt\n",
      "2023-06-13 11:41:51 INFO: Loading: pos\n",
      "2023-06-13 11:41:51 INFO: Loading: lemma\n",
      "2023-06-13 11:41:51 INFO: Loading: depparse\n",
      "2023-06-13 11:41:52 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='de', processors='tokenize,pos,lemma,depparse', download_method=None)\n",
    "doc = nlp(text[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\n",
       "   {\n",
       "     \"id\": 1,\n",
       "     \"text\": \"Die\",\n",
       "     \"lemma\": \"der\",\n",
       "     \"upos\": \"DET\",\n",
       "     \"xpos\": \"ART\",\n",
       "     \"feats\": \"Case=Nom|Definite=Def|Gender=Fem|Number=Plur|PronType=Art\",\n",
       "     \"head\": 2,\n",
       "     \"deprel\": \"det\",\n",
       "     \"start_char\": 0,\n",
       "     \"end_char\": 3\n",
       "   },\n",
       "   {\n",
       "     \"id\": 2,\n",
       "     \"text\": \"Wahlverwandtschaften\",\n",
       "     \"lemma\": \"Wahlverwandtschaft\",\n",
       "     \"upos\": \"NOUN\",\n",
       "     \"xpos\": \"NN\",\n",
       "     \"feats\": \"Case=Nom|Gender=Fem|Number=Plur\",\n",
       "     \"head\": 0,\n",
       "     \"deprel\": \"root\",\n",
       "     \"start_char\": 4,\n",
       "     \"end_char\": 24\n",
       "   }\n",
       " ]]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.sentences[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 1\tword: „\thead id: 7\thead: gesehen\tdeprel: punct\n",
      "id: 2\tword: Hast\thead id: 7\thead: gesehen\tdeprel: aux\n",
      "id: 3\tword: du\thead id: 7\thead: gesehen\tdeprel: nsubj\n",
      "id: 4\tword: meine\thead id: 5\thead: Frau\tdeprel: det:poss\n",
      "id: 5\tword: Frau\thead id: 7\thead: gesehen\tdeprel: obj\n",
      "id: 6\tword: nicht\thead id: 7\thead: gesehen\tdeprel: advmod\n",
      "id: 7\tword: gesehen\thead id: 10\thead: fragte\tdeprel: ccomp\n",
      "id: 8\tword: ?\thead id: 7\thead: gesehen\tdeprel: punct\n",
      "id: 9\tword: “\thead id: 7\thead: gesehen\tdeprel: punct\n",
      "id: 10\tword: fragte\thead id: 0\thead: root\tdeprel: root\n",
      "id: 11\tword: Eduard\thead id: 10\thead: fragte\tdeprel: nsubj\n",
      "id: 12\tword: ,\thead id: 17\thead: anschickte\tdeprel: punct\n",
      "id: 13\tword: indem\thead id: 17\thead: anschickte\tdeprel: mark\n",
      "id: 14\tword: er\thead id: 17\thead: anschickte\tdeprel: nsubj\n",
      "id: 15\tword: sich\thead id: 16\thead: weiterzugehen\tdeprel: obj\n",
      "id: 16\tword: weiterzugehen\thead id: 17\thead: anschickte\tdeprel: xcomp\n",
      "id: 17\tword: anschickte\thead id: 10\thead: fragte\tdeprel: advcl\n",
      "id: 18\tword: .\thead id: 10\thead: fragte\tdeprel: punct\n"
     ]
    }
   ],
   "source": [
    "print(*[f'id: {word.id}\\tword: {word.text}\\thead id: {word.head}\\thead: {sent.words[word.head-1].text if word.head > 0 else \"root\"}\\tdeprel: {word.deprel}' for sent in doc.sentences[8:9] for word in sent.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1', '„', '\"', 'PUNCT', '$(', '_', '7', 'punct', '_', 'start_char=533|end_char=534'], ['2', 'Hast', 'haben', 'AUX', 'VAFIN', 'Mood=Ind|Number=Sing|Person=2|Tense=Pres|VerbForm=Fin', '7', 'aux', '_', 'start_char=534|end_char=538'], ['3', 'du', 'du', 'PRON', 'PPER', 'Case=Nom|Number=Sing|Person=2|PronType=Prs', '7', 'nsubj', '_', 'start_char=539|end_char=541'], ['4', 'meine', 'mein', 'DET', 'PPOSAT', 'Case=Acc|Gender=Fem|Number=Sing|Number[psor]=Sing|Person=1|Poss=Yes|PronType=Prs', '5', 'det:poss', '_', 'start_char=542|end_char=547'], ['5', 'Frau', 'Frau', 'NOUN', 'NN', 'Case=Acc|Gender=Fem|Number=Sing', '7', 'obj', '_', 'start_char=548|end_char=552'], ['6', 'nicht', 'nicht', 'PART', 'PTKNEG', 'Polarity=Neg', '7', 'advmod', '_', 'start_char=553|end_char=558'], ['7', 'gesehen', 'sehen', 'VERB', 'VVPP', 'VerbForm=Part', '10', 'ccomp', '_', 'start_char=559|end_char=566'], ['8', '?', '?', 'PUNCT', '$.', '_', '7', 'punct', '_', 'start_char=566|end_char=567'], ['9', '“', '\"', 'PUNCT', '$(', '_', '7', 'punct', '_', 'start_char=567|end_char=568'], ['10', 'fragte', 'fragen', 'VERB', 'VVFIN', 'Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin', '0', 'root', '_', 'start_char=569|end_char=575'], ['11', 'Eduard', 'Eduard', 'PROPN', 'NE', 'Case=Nom|Gender=Masc|Number=Sing', '10', 'nsubj', '_', 'start_char=576|end_char=582'], ['12', ',', ',', 'PUNCT', '$,', '_', '17', 'punct', '_', 'start_char=582|end_char=583'], ['13', 'indem', 'indem', 'SCONJ', 'KOUS', '_', '17', 'mark', '_', 'start_char=584|end_char=589'], ['14', 'er', 'er', 'PRON', 'PPER', 'Case=Nom|Gender=Masc|Number=Sing|Person=3|PronType=Prs', '17', 'nsubj', '_', 'start_char=590|end_char=592'], ['15', 'sich', 'er|es|sie', 'PRON', 'PRF', 'Case=Acc|Number=Sing|Person=3|PronType=Prs|Reflex=Yes', '16', 'obj', '_', 'start_char=593|end_char=597'], ['16', 'weiterzugehen', 'weiterzugehen', 'VERB', 'VVIZU', 'VerbForm=Inf', '17', 'xcomp', '_', 'start_char=598|end_char=611'], ['17', 'anschickte', 'anschicken', 'VERB', 'VVFIN', 'Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin', '10', 'advcl', '_', 'start_char=612|end_char=622'], ['18', '.', '.', 'PUNCT', '$.', '_', '10', 'punct', '_', 'start_char=622|end_char=623']]\n"
     ]
    }
   ],
   "source": [
    "dicts = doc.to_dict() # dicts is List[List[Dict]], representing each token / word in each sentence in the document\n",
    "#dicts[0]\n",
    "\n",
    "from stanza.utils.conll import CoNLL\n",
    "conll = CoNLL.convert_dict(dicts) # conll is List[List[List]], representing each token / word in each sentence in the document\n",
    "print(conll[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t„\t\"\tPUNCT\t$(\t_\t7\tpunct\t_\tstart_char=533|end_char=534\n",
      "2\tHast\thaben\tAUX\tVAFIN\tMood=Ind|Number=Sing|Person=2|Tense=Pres|VerbForm=Fin\t7\taux\t_\tstart_char=534|end_char=538\n",
      "3\tdu\tdu\tPRON\tPPER\tCase=Nom|Number=Sing|Person=2|PronType=Prs\t7\tnsubj\t_\tstart_char=539|end_char=541\n",
      "4\tmeine\tmein\tDET\tPPOSAT\tCase=Acc|Gender=Fem|Number=Sing|Number[psor]=Sing|Person=1|Poss=Yes|PronType=Prs\t5\tdet:poss\t_\tstart_char=542|end_char=547\n",
      "5\tFrau\tFrau\tNOUN\tNN\tCase=Acc|Gender=Fem|Number=Sing\t7\tobj\t_\tstart_char=548|end_char=552\n",
      "6\tnicht\tnicht\tPART\tPTKNEG\tPolarity=Neg\t7\tadvmod\t_\tstart_char=553|end_char=558\n",
      "7\tgesehen\tsehen\tVERB\tVVPP\tVerbForm=Part\t10\tccomp\t_\tstart_char=559|end_char=566\n",
      "8\t?\t?\tPUNCT\t$.\t_\t7\tpunct\t_\tstart_char=566|end_char=567\n",
      "9\t“\t\"\tPUNCT\t$(\t_\t7\tpunct\t_\tstart_char=567|end_char=568\n",
      "10\tfragte\tfragen\tVERB\tVVFIN\tMood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\t0\troot\t_\tstart_char=569|end_char=575\n",
      "11\tEduard\tEduard\tPROPN\tNE\tCase=Nom|Gender=Masc|Number=Sing\t10\tnsubj\t_\tstart_char=576|end_char=582\n",
      "12\t,\t,\tPUNCT\t$,\t_\t17\tpunct\t_\tstart_char=582|end_char=583\n",
      "13\tindem\tindem\tSCONJ\tKOUS\t_\t17\tmark\t_\tstart_char=584|end_char=589\n",
      "14\ter\ter\tPRON\tPPER\tCase=Nom|Gender=Masc|Number=Sing|Person=3|PronType=Prs\t17\tnsubj\t_\tstart_char=590|end_char=592\n",
      "15\tsich\ter|es|sie\tPRON\tPRF\tCase=Acc|Number=Sing|Person=3|PronType=Prs|Reflex=Yes\t16\tobj\t_\tstart_char=593|end_char=597\n",
      "16\tweiterzugehen\tweiterzugehen\tVERB\tVVIZU\tVerbForm=Inf\t17\txcomp\t_\tstart_char=598|end_char=611\n",
      "17\tanschickte\tanschicken\tVERB\tVVFIN\tMood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\t10\tadvcl\t_\tstart_char=612|end_char=622\n",
      "18\t.\t.\tPUNCT\t$.\t_\t10\tpunct\t_\tstart_char=622|end_char=623\n"
     ]
    }
   ],
   "source": [
    "tree_string = '\\n'.join(['\\t'.join(x) for x in conll[8]])\n",
    "print(tree_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: G Pages: 1 -->\n",
       "<svg width=\"986pt\" height=\"388pt\"\n",
       " viewBox=\"0.00 0.00 986.22 388.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 384)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-384 982.2227,-384 982.2227,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<text text-anchor=\"middle\" x=\"628\" y=\"-357.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">0 (None)</text>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>10</title>\n",
       "<text text-anchor=\"middle\" x=\"628\" y=\"-271.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">10 (fragte)</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;10 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M628,-343.7616C628,-332.3597 628,-317.4342 628,-304.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"631.5001,-304.2121 628,-294.2121 624.5001,-304.2121 631.5001,-304.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"639.2759\" y=\"-314.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">root</text>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>7</title>\n",
       "<text text-anchor=\"middle\" x=\"316\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">7 (gesehen)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;7 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>10&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M590.0546,-265.5407C533.943,-250.074 428.7584,-221.0808 366.2866,-203.861\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"367.2116,-200.4855 356.641,-201.2023 365.3514,-207.2339 367.2116,-200.4855\"/>\n",
       "<text text-anchor=\"middle\" x=\"509.6587\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">ccomp</text>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>11</title>\n",
       "<text text-anchor=\"middle\" x=\"573\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">11 (Eduard)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;11 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>10&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M615.4605,-257.7903C611.6213,-252.1253 607.4209,-245.8348 603.6621,-240 598.9317,-232.6568 593.9286,-224.6208 589.3668,-217.1807\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"592.0703,-214.891 583.8783,-208.1723 586.0925,-218.5332 592.0703,-214.891\"/>\n",
       "<text text-anchor=\"middle\" x=\"619.1689\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 17 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>17</title>\n",
       "<text text-anchor=\"middle\" x=\"683\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">17 (anschickte)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;17 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>10&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M639.6641,-257.7616C647.2477,-245.9036 657.2686,-230.2345 665.7639,-216.951\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"668.9135,-218.5223 671.3527,-208.2121 663.0163,-214.7509 668.9135,-218.5223\"/>\n",
       "<text text-anchor=\"middle\" x=\"674.1587\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advcl</text>\n",
       "</g>\n",
       "<!-- 18 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>18</title>\n",
       "<text text-anchor=\"middle\" x=\"779\" y=\"-185.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">18 (.)</text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;18 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>10&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M661.1416,-257.8187C671.3808,-252.1548 682.6785,-245.8578 693,-240 709.5738,-230.5937 727.8377,-220.0068 743.2555,-211.0047\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"745.1047,-213.9779 751.9701,-205.9084 741.5709,-207.9353 745.1047,-213.9779\"/>\n",
       "<text text-anchor=\"middle\" x=\"732.5518\" y=\"-228.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">punct</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>1</title>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">1 („)</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2</title>\n",
       "<text text-anchor=\"middle\" x=\"103\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">2 (Hast)</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>3</title>\n",
       "<text text-anchor=\"middle\" x=\"179\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">3 (du)</text>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>4</title>\n",
       "<text text-anchor=\"middle\" x=\"255\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">4 (meine)</text>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>5</title>\n",
       "<text text-anchor=\"middle\" x=\"255\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">5 (Frau)</text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;4 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>5&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M255,-85.7616C255,-74.3597 255,-59.4342 255,-46.494\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"258.5001,-46.2121 255,-36.2121 251.5001,-46.2121 258.5001,-46.2121\"/>\n",
       "<text text-anchor=\"middle\" x=\"277.9448\" y=\"-56.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">det:poss</text>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>6</title>\n",
       "<text text-anchor=\"middle\" x=\"336\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">6 (nicht)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;1 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>7&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M275.3349,-184.0909C239.063,-178.2663 184.7608,-168.1762 138.8965,-154 106.654,-144.0342 97.6192,-138.2543 63.3279,-122.0936\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"64.5775,-118.8144 54.036,-117.7525 61.6145,-125.1564 64.5775,-118.8144\"/>\n",
       "<text text-anchor=\"middle\" x=\"154.5518\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">punct</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;2 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>7&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M275.3486,-178.3751C254.0681,-171.9063 227.7344,-163.2946 204.7861,-154 184.0238,-145.5908 161.6111,-134.7799 143.1161,-125.3633\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"144.5978,-122.1896 134.1043,-120.7281 141.3961,-128.4145 144.5978,-122.1896\"/>\n",
       "<text text-anchor=\"middle\" x=\"215.1069\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">aux</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;3 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>7&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M279.2764,-171.8987C268.8833,-166.4434 257.6782,-160.2396 247.6621,-154 235.0725,-146.1572 221.7761,-136.7585 210.2718,-128.2372\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"212.3482,-125.4196 202.2473,-122.2223 208.1497,-131.0208 212.3482,-125.4196\"/>\n",
       "<text text-anchor=\"middle\" x=\"263.1689\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;5 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>7&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M303.0635,-171.7616C294.5717,-159.7896 283.3244,-143.9328 273.8451,-130.5685\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"276.5581,-128.3437 267.9179,-122.2121 270.8486,-132.3935 276.5581,-128.3437\"/>\n",
       "<text text-anchor=\"middle\" x=\"298.9448\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">obj</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;6 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>7&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M320.2415,-171.7616C322.9196,-160.2456 326.4336,-145.1353 329.4636,-132.1064\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"332.9084,-132.745 331.7646,-122.2121 326.0904,-131.1594 332.9084,-132.745\"/>\n",
       "<text text-anchor=\"middle\" x=\"350.5518\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">advmod</text>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>8</title>\n",
       "<text text-anchor=\"middle\" x=\"413\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">8 (?)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;8 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>7&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M352.2317,-171.9467C360.8363,-166.7861 369.6442,-160.716 377,-154 384.4253,-147.2205 391.2514,-138.6784 396.9172,-130.5821\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"399.9522,-132.3392 402.5833,-122.0763 394.1264,-128.4584 399.9522,-132.3392\"/>\n",
       "<text text-anchor=\"middle\" x=\"405.5518\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">punct</text>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>9</title>\n",
       "<text text-anchor=\"middle\" x=\"485\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">9 (“)</text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;9 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>7&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M356.5976,-180.5091C377.7808,-174.6035 403.6564,-165.8511 425,-154 437.1603,-147.248 449.2088,-137.8791 459.2828,-129.0746\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"461.8256,-131.4943 466.903,-122.1953 457.1349,-126.2984 461.8256,-131.4943\"/>\n",
       "<text text-anchor=\"middle\" x=\"460.5518\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">punct</text>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>12</title>\n",
       "<text text-anchor=\"middle\" x=\"557\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">12 (,)</text>\n",
       "</g>\n",
       "<!-- 17&#45;&gt;12 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>17&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M652.8497,-171.9529C643.7685,-166.3481 633.8411,-160.0503 624.8965,-154 612.915,-145.8955 600.0673,-136.5982 588.8071,-128.2363\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"590.6512,-125.2449 580.5465,-122.0595 586.4593,-130.851 590.6512,-125.2449\"/>\n",
       "<text text-anchor=\"middle\" x=\"640.5518\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">punct</text>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>13</title>\n",
       "<text text-anchor=\"middle\" x=\"641\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">13 (indem)</text>\n",
       "</g>\n",
       "<!-- 17&#45;&gt;13 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>17&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M674.0929,-171.7616C668.3574,-160.0176 660.7964,-144.5355 654.3496,-131.3349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"657.4277,-129.6618 649.8943,-122.2121 651.1377,-132.7337 657.4277,-129.6618\"/>\n",
       "<text text-anchor=\"middle\" x=\"679.3828\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">mark</text>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>14</title>\n",
       "<text text-anchor=\"middle\" x=\"725\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">14 (er)</text>\n",
       "</g>\n",
       "<!-- 17&#45;&gt;14 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>17&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M691.9071,-171.7616C697.6426,-160.0176 705.2036,-144.5355 711.6504,-131.3349\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"714.8623,-132.7337 716.1057,-122.2121 708.5723,-129.6618 714.8623,-132.7337\"/>\n",
       "<text text-anchor=\"middle\" x=\"722.1689\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">nsubj</text>\n",
       "</g>\n",
       "<!-- 15 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>15</title>\n",
       "<text text-anchor=\"middle\" x=\"803\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">15 (sich)</text>\n",
       "</g>\n",
       "<!-- 17&#45;&gt;15 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>17&#45;&gt;15</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M713.8614,-171.8201C722.7663,-166.3143 732.3975,-160.1035 741,-154 752.1235,-146.1078 763.8792,-136.8748 774.1286,-128.5059\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"776.5004,-131.0865 781.9846,-122.0215 772.0444,-125.6879 776.5004,-131.0865\"/>\n",
       "<text text-anchor=\"middle\" x=\"767.9448\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">obj</text>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>16</title>\n",
       "<text text-anchor=\"middle\" x=\"916\" y=\"-99.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">16 (weiterzugehen)</text>\n",
       "</g>\n",
       "<!-- 17&#45;&gt;16 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>17&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M732.1109,-171.9883C747.7414,-166.248 765.0998,-159.8649 781,-154 806.1436,-144.7256 833.9084,-134.4485 857.6875,-125.6358\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"859.1118,-128.8406 867.2719,-122.083 856.6788,-122.277 859.1118,-128.8406\"/>\n",
       "<text text-anchor=\"middle\" x=\"838.0518\" y=\"-142.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">xcomp</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<DependencyGraph with 19 nodes>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import DependencyGraph\n",
    "t = DependencyGraph(tree_string)\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 3 (Weiterverarbeitung Korpusannotation)\n",
    "\n",
    "Führen Sie auf dem Wahlverwandschaften-Text mit stanza ein POS-Tagging aus und verwenden Sie die Ausgabe für eine POS-Frequenzzählung und Plotting der Ergebnisse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-13 11:42:03 INFO: Loading these models for language: de (German):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "| pos       | gsd     |\n",
      "=======================\n",
      "\n",
      "2023-06-13 11:42:03 INFO: Using device: cpu\n",
      "2023-06-13 11:42:03 INFO: Loading: tokenize\n",
      "2023-06-13 11:42:03 INFO: Loading: mwt\n",
      "2023-06-13 11:42:03 INFO: Loading: pos\n",
      "2023-06-13 11:42:03 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "text = open('../wahlverwandschaften.txt').read()\n",
    "nlp = stanza.Pipeline(lang='de', processors='tokenize, mwt, pos', download_method=None)\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{\n",
       "   \"id\": 1,\n",
       "   \"text\": \"Die\",\n",
       "   \"upos\": \"DET\",\n",
       "   \"xpos\": \"ART\",\n",
       "   \"feats\": \"Case=Nom|Definite=Def|Gender=Fem|Number=Plur|PronType=Art\",\n",
       "   \"start_char\": 0,\n",
       "   \"end_char\": 3\n",
       " },\n",
       " {\n",
       "   \"id\": 2,\n",
       "   \"text\": \"Wahlverwandtschaften\",\n",
       "   \"upos\": \"NOUN\",\n",
       "   \"xpos\": \"NN\",\n",
       "   \"feats\": \"Case=Nom|Gender=Fem|Number=Plur\",\n",
       "   \"start_char\": 4,\n",
       "   \"end_char\": 24\n",
       " }]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.sentences[0].words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Die', 'DET'),\n",
       " ('Wahlverwandtschaften', 'NOUN'),\n",
       " ('Ein', 'DET'),\n",
       " ('Roman', 'NOUN'),\n",
       " ('von', 'ADP')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_text = [(word.text,word.upos) for sent in doc.sentences for word in sent.words]\n",
    "tagged_text[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "pos_count = Counter(tag for _, tag in tagged_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['DET', 'NOUN', 'ADP', 'PROPN', 'ADJ', 'ADV', 'VERB', 'PRON', 'AUX', 'PUNCT', 'PART', 'CCONJ', 'SCONJ', 'NUM'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_count.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([1190, 1721, 915, 195, 902, 1287, 1525, 1841, 557, 2330, 276, 422, 314, 12])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_count.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [10, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "height = pos_count.values()\n",
    "bars = pos_count.keys()\n",
    "\n",
    "y_pos = np.arange(len(bars))\n",
    "\n",
    "# Create bars\n",
    "plt.bar(y_pos, height)\n",
    "\n",
    "# Create names on the x-axis\n",
    "plt.xticks(y_pos, bars)\n",
    "\n",
    "# Show graphic\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PUNCT', 2330),\n",
       " ('PRON', 1841),\n",
       " ('NOUN', 1721),\n",
       " ('VERB', 1525),\n",
       " ('ADV', 1287),\n",
       " ('DET', 1190),\n",
       " ('ADP', 915),\n",
       " ('ADJ', 902),\n",
       " ('AUX', 557),\n",
       " ('CCONJ', 422),\n",
       " ('SCONJ', 314),\n",
       " ('PART', 276),\n",
       " ('PROPN', 195),\n",
       " ('NUM', 12)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_pos_count = pos_count.most_common()\n",
    "sorted_pos_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2330, 1841, 1721, 1525, 1287, 1190, 915, 902, 557, 422, 314, 276, 195, 12]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[count for _, count in sorted_pos_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "height = [count for _, count in sorted_pos_count]\n",
    "bars = [tag for tag, _ in sorted_pos_count]\n",
    "\n",
    "y_pos = np.arange(len(bars))\n",
    "\n",
    "# Create bars\n",
    "plt.bar(y_pos, height)\n",
    "\n",
    "# Create names on the x-axis\n",
    "plt.xticks(y_pos, bars)\n",
    "\n",
    "# Show graphic\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 4: Tagging mit NLTK\n",
    "\n",
    "Auch NLTK enthält Modelle für Textannotationen. Testen Sie die in den NLTK-Kapiteln 3 und 5 beschriebenen Tagger (**POS, Segmentizer, Stemmer, Lemmatizer**) für das Englische aus (wie man einen POS-Tagger mit NLTK selbst trainiert, um etwa auch auf deutschen Texten POS-Tagging mit NLTK durchzuführen, ist Thema in einer späteren Sitzung).\n",
    "\n",
    "- https://www.nltk.org/book/ch03.html\n",
    "- https://www.nltk.org/book/ch05.html\n",
    "\n",
    "\n",
    "> HINWEIS: NLTK ist nicht primär Annotationstool wie stanza/spacy, sondern eher für Preprocessing und trainieren eigener Modelle geeignet (nur wenige vortrainierte Sprachmodelle in NLTK enthalten).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS-Tagging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('And', 'CC'),\n",
       " ('now', 'RB'),\n",
       " ('for', 'IN'),\n",
       " ('something', 'NN'),\n",
       " ('completely', 'RB'),\n",
       " ('different', 'JJ'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zb: pos-tagging\n",
    "#from nltk.tokenize import word_tokenize\n",
    "text = nltk.word_tokenize(\"And now for something completely different.\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Die', 'NNP'),\n",
       " ('Wahlverwandtschaften', 'NNP'),\n",
       " ('Ein', 'NNP'),\n",
       " ('Roman', 'NNP'),\n",
       " ('von', 'NNP'),\n",
       " ('Johann', 'NNP'),\n",
       " ('Wolfgang', 'NNP'),\n",
       " ('von', 'NNP'),\n",
       " ('Goethe', 'NNP'),\n",
       " ('Erster', 'NNP')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pos_tag basiert auf englischem Modell, kein sinnvolles Ergbenis z.B. für deutsche Texte):\n",
    "text = nltk.word_tokenize(open('../wahlverwandschaften.txt').read())\n",
    "#text[0:10]\n",
    "nltk.pos_tag(text[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('die', 'NN'),\n",
       " ('wahlverwandtschaften', 'WRB'),\n",
       " ('ein', 'JJ'),\n",
       " ('roman', 'NN'),\n",
       " ('von', 'NN'),\n",
       " ('johann', 'NN'),\n",
       " ('wolfgang', 'NN'),\n",
       " ('von', 'IN'),\n",
       " ('goethe', 'NN'),\n",
       " ('erster', 'NN')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Case relevant für POS-Tagging!\n",
    "text = nltk.word_tokenize(open('../wahlverwandschaften.txt').read().lower())\n",
    "#text[0:10]\n",
    "nltk.pos_tag(text[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Segmentation:\n",
    "- https://www.nltk.org/book/ch03.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' And now.', 'Something completely different.']\n"
     ]
    }
   ],
   "source": [
    "raw_text = \" And now. Something completely different.\"\n",
    "\n",
    "sents = nltk.sent_tokenize(raw_text)\n",
    "print(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK-Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "is no basis for a system of government.  Supreme executive power derives from\n",
    "a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "tokens = word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['denni', ':', 'listen', ',', 'strang', 'women', 'lie', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'basi', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'power', 'deriv', 'from', 'a', 'mandat', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcic', 'aquat', 'ceremoni', '.']\n"
     ]
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "print([porter.stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['den', ':', 'list', ',', 'strange', 'wom', 'lying', 'in', 'pond', 'distribut', 'sword', 'is', 'no', 'bas', 'for', 'a', 'system', 'of', 'govern', '.', 'suprem', 'execut', 'pow', 'der', 'from', 'a', 'mand', 'from', 'the', 'mass', ',', 'not', 'from', 'som', 'farc', 'aqu', 'ceremony', '.']\n"
     ]
    }
   ],
   "source": [
    "print([lancaster.stem(t) for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK-Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DENNIS', ':', 'Listen', ',', 'strange', 'woman', 'lying', 'in', 'pond', 'distributing', 'sword', 'is', 'no', 'basis', 'for', 'a', 'system', 'of', 'government', '.', 'Supreme', 'executive', 'power', 'derives', 'from', 'a', 'mandate', 'from', 'the', 'mass', ',', 'not', 'from', 'some', 'farcical', 'aquatic', 'ceremony', '.']\n"
     ]
    }
   ],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "print([wnl.lemmatize(t) for t in tokens])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
